ä¸­æ–‡ README æ–‡ä»¶è§ [README in Chinese](./README_Chinese.md).

# LLM-Spectro-Agent

An LLM-powered agent for human-like analysis of one-dimensional astronomical spectra.

## Overview

This project use large language models (LLMs) to perform human-like astrophysical inference on 1D spectra, specifically:
- **Source classification** (Only support star, galaxy, QSO)
- **Redshift estimation**

The system mimics the cognitive workflow of a human astronomer:
1. **Visual interpretation** of the spectrum plot (axes, units, features)
2. **Rule-based analysis** using astrophysical knowledge (e.g., LyÎ±, C IV, Mg II lines)
3. **Multi-agent debate** between an auditor and refinement assistant to improve robustness
4. **Synthesis** of a final report with confidence assessment

The pipeline is currently configured to use the following Qwen models via API:
- **Text reasoning**: `qwen3-max-2025-09-23`
- **Visual understanding**: `qwen-vl-max-2025-08-13`

> âš ï¸ Note: Other LLMs have not been tested and may require adaptation.

---

## Dependencies & Installation

### 1. Python Dependencies
Install the required Python packages:
```bash
pip install -r requirements.txt
```

### 2. System Dependencies
This project relies on **Tesseract OCR** for text detection in spectrum plots. Install it based on your OS:

- **Ubuntu/Debian**:
  ```bash
  sudo apt-get install tesseract-ocr
  ```

- **macOS** (with Homebrew):
  ```bash
  brew install tesseract
  ```

- **Windows**:  
  Download and install from [UB Mannheim Tesseract](https://github.com/UB-Mannheim/tesseract/wiki)

> ğŸ“Œ Make sure `tesseract` is in your system PATH. Verify with:
> ```bash
> tesseract --version
> ```

### 3. Environment Setup
Copy the example configuration and fill in your settings:
```bash
cp .env_example .env
```

Edit `.env` to specify:
- `API_KEY`: Your API key for LLM models
- `INPUT_DIR`, `OUTPUT_DIR`: Input and output directories
- `IMAGE_NAME`: Name of the spectrum image (without `.png` extension)
- and other parameters

---

## Quick Start

### 1. Run the Analysis
Execute the main script:
```bash
python main.py
```
Results will be saved to the output directory specified in `.env`.

### 2. Try the Notebook (Optional)
For interactive exploration and debugging, see `debug2.ipynb`. After setting up your environment variables, you can run this notebook to step through the analysis pipeline.

---

## Test set
A basic test set is offered in ./data/test_set

---

## Output Files

For an input image named `{your_image_name}.png`, the program generates the following outputs in the configured output directory:

- `{your_image_name}_cropped.png`  
  Cleaned spectrum image with titles, axes, and borders removed.

- `{your_image_name}_reconstructed.png`  
  Reconstructed spectrum after OpenCV-based preprocessing.

- `{your_image_name}_features.png`  
  Visualization of detected peaks and troughs.

- `{your_image_name}_rule_analysis.md`  
  Intermediate rule-based analysis from the spectral analyst agent.

- `{your_image_name}_summary.md`  
  Final synthesized report including source type, redshift estimate, and confidence assessment.

---

## Architecture Highlights

- **Multi-Agent Debate**: Uses LangGraph to orchestrate a structured debate between an auditor and refinement assistant, enhancing result reliability.
- **Multi-Scale Feature Detection**: Detects spectral features across multiple Gaussian smoothing scales and merges them robustly.
- **Hybrid Vision + Language**: Combines computer vision (OpenCV, OCR) with multimodal LLMs for end-to-end spectrum understanding.
- **MCP Integration**: Built on the Model Context Protocol (MCP) for standardized tool interaction.

---

## Requirements

```txt
# Core scientific computing
numpy>=1.21.0
scipy>=1.7.0
pandas>=1.3.0

# Computer vision & OCR
opencv-python>=4.5.0
pytesseract>=0.3.8

# Plotting
matplotlib>=3.4.0

# Language model & agent framework
langchain-openai>=0.1.0
langgraph>=0.1.0
langchain-core>=0.1.0

# MCP (Model Context Protocol) integration
langchain-mcp-adapters>=0.1.0
mcp>=1.19.0

# Utilities
python-dotenv>=0.19.0
openai>=1.0.0
pydantic>=2.0.0
```
---

## License

This project is for research and educational purposes. 

---

## 10.29 æ›´æ–°
- æ–°å¢ main.py
  - ä½¿ç”¨æ–¹å¼ï¼š
    - é…ç½® .env æ–‡ä»¶ã€‚ç¤ºä¾‹ä¸º .env_example
    - è€Œåè¿è¡Œ
      ```bash
      python main.py
      ```

- ä½¿ç”¨ langgraph å¯¹ æµç¨‹è¿›è¡Œäº†æ”¹å†™ã€‚æ—§çš„ astro_agents ç¨‹åºåœ¨ src/_astro_agents_old.py ä¸­
- æ–°å¢ src/workflow_orchestrator.pyï¼Œç”¨æ¥ç®¡ç† agent çš„è¿è¡Œæµç¨‹

## 10.24 æ›´æ–°
- ä½¿ç”¨ç¯å¢ƒå˜é‡ä½œä¸ºå‚æ•°çš„è¾“å…¥æ–¹å¼
- ç¯å¢ƒå˜é‡çš„é…ç½®åœ¨ .env æ–‡ä»¶ä¸­ã€‚.env æ–‡ä»¶çš„é…ç½®ç¤ºä¾‹åœ¨ .env_example æ–‡ä»¶é‡Œã€‚
- æ¥å—è¿™äº›è¾“å…¥å‚æ•°çš„ä½ç½®ï¼š
  - debug.ipynb çš„åˆå§‹åŒ–é˜¶æ®µï¼Œæ¥å—
    - input_dir = os.getenv('INPUT_DIR')
    - output_dir = os.getenv('OUTPUT_DIR')
    - SINGLE_RUN = os.getenv('SINGLE_RUN').lower()=='true'
    - image_name = os.getenv('IMAGE_NAME')
    - IMAGE_NAME_HEADERã€STARTã€ENDè¿™ä¸‰ä¸ªå‚æ•°æ˜¯æ‰¹é‡å¤„ç†æ‰€ä½¿ç”¨çš„å‚æ•°ï¼Œæš‚æœªå®è£…
  - src/mcp_manager._init_llm() æ¥å—ä¸¤ç§ LLM çš„å‚æ•°ã€‚llm_type='LLM' æˆ– 'VIS_LLM'.
    - api_key = self._get_env_or_raise(f"{llm_type}_API_KEY")
    - base_url = self._get_env_or_raise(f"{llm_type}_BASE_URL").rstrip()
    - model = os.getenv(f"{llm_type}_MODEL", default_model)
    - temp_str = os.getenv(f"{llm_type}_TEMPERATURE", "0.1")
    - temperature = float(temp_str) if temp_str else 0.1
    - max_tokens_str = os.getenv(f"{llm_type}_MAX_TOKENS")
  - src/astro_agent.SpectralVisualInterpreter.run() æ¥å—
    - SIGMA_LIST
    - TOL_PIXELS
    - WEIGHT_ORIGINAL
    - PROM_THRESHOLD_PEAKS
    - PROM_THRESHOLD_TROUGHS
    - ä»¥åŠ 
      - p_ = os.getenv('PLOT_PEAKS_NUMBER')
      - t_ = os.getenv('PLOT_TROUGHS_NUMBER')

- ä¸‹ä¸€æ­¥è®¡åˆ’ï¼š
  - [x] å°† debug.ipynb ä¸­ç›®å‰çš„è¿è¡Œæµç¨‹å°è£…åˆ° src.workflow_orchestrator
  - [x] æœ€ç»ˆçš„ main.py ä¸­å¯èƒ½åªåŒ…æ‹¬ debug.ipynb ä¸­çš„åˆå§‹åŒ–é˜¶æ®µ + å¯¹ workflow_orchestrator å‡½æ•°çš„è°ƒç”¨ã€‚