import json
import os
import numpy as np
import matplotlib.pyplot as plt
import logging

from scipy.ndimage import gaussian_filter1d

from .context_manager import SpectroState
from .base_agent import BaseAgent
from .mcp_manager import MCPManager

from .utils import (
    _detect_axis_ticks, _detect_chart_border, _crop_img,
    _remap_to_cropped_canvas, _pixel_tickvalue_fitting,
    _process_and_extract_curve_points, _convert_to_spectrum,
    _find_features_multiscale, _plot_spectrum, _plot_features,
    parse_list, getenv_float, getenv_int, _load_feature_params, 
    _ROI_features_finding, merge_features, plot_cleaned_features, 
    safe_to_bool, find_overlap_regions
)

# ---------------------------------------------------------
# 1. Visual Assistant â€” è´Ÿè´£å›¾åƒç†è§£ä¸Žåæ ‡é˜…è¯»
# ---------------------------------------------------------
class SpectralVisualInterpreter(BaseAgent):
    """
    SpectralVisualInterpreter

    ä»Žç§‘å­¦å…‰è°±å›¾ä¸­è‡ªåŠ¨æå–åæ ‡è½´åˆ»åº¦ã€è¾¹æ¡†ã€åƒç´ æ˜ å°„ã€å³°/è°·ç­‰ä¿¡æ¯
    """

    def __init__(self, mcp_manager: MCPManager):
        super().__init__(
            agent_name='Spectral Visual Interpreter',
            mcp_manager=mcp_manager
        )

    # --------------------------
    # Step 1.1: æ£€æµ‹åæ ‡è½´åˆ»åº¦
    # --------------------------
    async def detect_axis_ticks(self, state: SpectroState):
        """
        è°ƒç”¨è§†è§‰ LLM æ£€æµ‹åæ ‡è½´åˆ»åº¦ï¼Œå¦‚æžœæ— å›¾åƒæˆ–éžå…‰è°±å›¾æŠ¥é”™
        """
        class NoImageError(Exception): pass
        class NotSpectralImageError(Exception): pass

        if not state['image_path'] or not os.path.exists(state['image_path']):
            print(state['image_path'])
            raise NoImageError("âŒ æœªè¾“å…¥å›¾åƒæˆ–å›¾åƒè·¯å¾„ä¸å­˜åœ¨")

        system_prompt = state['prompt'][f'{self.agent_name}']['detect_axis_ticks']['system_prompt']
        user_prompt = state['prompt'][f'{self.agent_name}']['detect_axis_ticks']['user_prompt']

        axis_info = await self.call_llm_with_context(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            image_path=state['image_path'],
            parse_json=True,
            description="åæ ‡è½´ä¿¡æ¯"
        )
        if axis_info == "éžå…‰è°±å›¾":
            raise NotSpectralImageError(f"âŒ å›¾åƒä¸æ˜¯å…‰è°±å›¾ï¼ŒLLM è¾“å‡º: {axis_info}")
        # print(axis_info)
        state["axis_info"] = axis_info

    # --------------------------
    # Step 1.2~1.3: åˆå¹¶è§†è§‰+OCRåˆ»åº¦
    # --------------------------
    async def combine_axis_mapping(self, state: SpectroState):
        """ç»“åˆè§†è§‰ç»“æžœä¸Ž OCR ç»“æžœç”Ÿæˆåƒç´ -æ•°å€¼æ˜ å°„"""
        axis_info_json = json.dumps(state['axis_info'], ensure_ascii=False)
        ocr_json = json.dumps(state['OCR_detected_ticks'], ensure_ascii=False)

        system_prompt = state['prompt'][f'{self.agent_name}']['combine_axis_mapping']['system_prompt']
        user_prompt = state['prompt'][f'{self.agent_name}']['combine_axis_mapping']['user_prompt'].format(
            axis_info_json=axis_info_json,
            ocr_json=ocr_json
        )
        tick_pixel_raw = await self.call_llm_with_context(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            image_path=None,
            parse_json=True,
            description="åˆ»åº¦-åƒç´ æ˜ å°„"
        )
        state["tick_pixel_raw"] = tick_pixel_raw
        print(tick_pixel_raw)

    # --------------------------
    # Step 1.4: æ ¡éªŒä¸Žä¿®æ­£
    # --------------------------
    async def revise_axis_mapping(self, state: SpectroState):
        """æ£€æŸ¥å¹¶ä¿®æ­£åˆ»åº¦å€¼ä¸Žåƒç´ ä½ç½®åŒ¹é…å…³ç³»"""
        axis_mapping_json = json.dumps(state['tick_pixel_raw'], ensure_ascii=False)

        system_prompt = state['prompt'][f'{self.agent_name}']['revise_axis_mapping']['system_prompt']
        user_prompt = state['prompt'][f'{self.agent_name}']['revise_axis_mapping']['user_prompt'].format(
            axis_mapping_json=axis_mapping_json
        )

        tick_pixel_revised = await self.call_llm_with_context(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            image_path=None,
            parse_json=True,
            description="ä¿®æ­£åŽçš„åˆ»åº¦æ˜ å°„"
        )
        state["tick_pixel_raw"] = tick_pixel_revised
        # print(tick_pixel_revised)

    # --------------------------
    # Step 1.5 å›¾åƒè£å‰ª
    # --------------------------
    async def check_border(self, state):
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘å­¦å›¾è¡¨åˆ†æžåŠ©æ‰‹ï¼Œä¸“æ³¨äºŽå¤„ç†å¤©æ–‡å­¦é¢†åŸŸçš„ matplotlib å…‰è°±å›¾ã€‚ä½ å…·å¤‡è¯†åˆ«å›¾åƒè¾¹ç¼˜æ˜¯å¦æ®‹ç•™åæ ‡è½´è¾¹æ¡†æˆ–è£…é¥°æ€§ç›´çº¿çš„èƒ½åŠ›ï¼Œå¹¶èƒ½åŸºäºŽè§†è§‰å†…å®¹åšå‡ºç²¾å‡†åˆ¤æ–­ã€‚
"""
        user_prompt = """
ä½ å°†æŽ¥æ”¶åˆ°ä¸¤å¼ å›¾åƒï¼š
- ä¸€å¼ æ˜¯åŽŸå§‹å…‰è°±å›¾åƒï¼Œå¯èƒ½å¸¦æœ‰ç»˜å›¾è¾¹æ¡†ã€‚
- ä¸€å¼ æ˜¯ç»è¿‡ OCR ä¸Ž OpenCV é¢„å¤„ç†åŽçš„ matplotlib å¤©æ–‡å­¦å…‰è°±å›¾ã€‚å·²å°è¯•è£å‰ªæŽ‰åŽŸå§‹å›¾è¡¨çš„è¾¹æ¡†åŠå…¶å¤–éƒ¨åŒºåŸŸã€‚

è¯·åˆ¤æ–­å›¾åƒå››æ¡è¾¹ç¼˜ï¼ˆä¸Šã€å³ã€ä¸‹ã€å·¦ï¼‰æ˜¯å¦ä»æ®‹ç•™æœ‰æ˜Žæ˜¾çš„ç›´çº¿åž‹è¾¹æ¡†ç—•è¿¹ï¼ˆä¾‹å¦‚ï¼šé•¿è€Œç›´çš„é»‘è‰²æˆ–æ·±è‰²çº¿æ®µï¼Œé€šå¸¸ä¸ºåæ ‡è½´å¤–æ¡†çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æžœæŸä¸€è¾¹ç¼˜**å®Œå…¨çœ‹ä¸åˆ°**æ­¤ç±»ç›´çº¿æ®µï¼Œåˆ™è§†ä¸ºâ€œè£å‰ªå¹²å‡€â€ã€‚
- å¦‚æžœæŸä¸€è¾¹ç¼˜**ä»å¯è§**æ˜Žæ˜¾çš„ç›´çº¿æ®µï¼ˆå³ä½¿å¾ˆç»†ï¼‰ï¼Œåˆ™è§†ä¸ºâ€œæœªè£å‰ªå¹²å‡€â€ã€‚

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºç»“æžœï¼Œä»…åŒ…å«å››ä¸ªé”®ï¼Œå€¼å¿…é¡»ä¸ºå­—ç¬¦ä¸² 'true'ï¼ˆè¡¨ç¤ºå¹²å‡€ï¼‰æˆ– 'false'ï¼ˆè¡¨ç¤ºä¸å¹²å‡€ï¼‰ï¼š

{
    "top": "true" or "false",
    "right": "true" or "false",
    "bottom": "true" or "false",
    "left": "true" or "false"
}

ä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚
"""
        response = await self.call_llm_with_context(
            system_prompt,
            user_prompt,
            image_path=[state['image_path'], state['crop_path']],
            parse_json=True,
            description='æ£€æŸ¥è£å‰ª'
        )
        try:
            response['top'] = safe_to_bool(response['top'])
            response['right'] = safe_to_bool(response['right'])
            response['bottom'] = safe_to_bool(response['bottom'])
            response['left'] = safe_to_bool(response['left'])
            return response
        except:
            logging.error(f"LLM è¾“å‡ºæ ¼å¼é”™è¯¯: {response}")

    async def peak_trough_detection(self, state: SpectroState):
        try:
            sigma_list, tol_pixels, prom_peaks, prom_troughs, weight_original, _, _ = _load_feature_params()
            state['sigma_list'] = sigma_list

            spec = state["spectrum"]
            wavelengths = np.array(spec["new_wavelength"])
            flux = np.array(spec["weighted_flux"])

            state["peaks"] = _find_features_multiscale(
                wavelengths, flux,
                state, feature="peak", sigma_list=sigma_list,
                prom=prom_peaks, tol_pixels=tol_pixels, weight_original=weight_original,
                use_continuum_for_trough=True
            )
            state["troughs"] = _find_features_multiscale(
                wavelengths, flux,
                state, feature="trough", sigma_list=sigma_list,
                prom=prom_troughs, tol_pixels=tol_pixels, weight_original=weight_original,
                use_continuum_for_trough=True,
                min_depth=0.08
            )
            # print(len(state["peaks"]), len(state["troughs"]))

            # æŠŠwavelengthsæŒ‰ç…§æ¯500åŸƒä¸ºä¸€ä¸ªROIè¿›è¡Œåˆ’åˆ†ï¼Œåˆ†åˆ«è¿›è¡Œå³°è°·æ£€æµ‹
            ROI_peaks = []
            ROI_troughs = []
            roi_size = 500  # æ¯ä¸ªROIçš„å®½åº¦ï¼Œå•ä½ä¸ºåŸƒ
            roi_edges = np.arange(wavelengths[0], wavelengths[-1], roi_size)
            for i in range(len(roi_edges)-1):
                roi_start = roi_edges[i]
                roi_end = roi_edges[i+1]
                mask = (wavelengths >= roi_start) & (wavelengths < roi_end)
                roi_wavelengths = np.where(mask, wavelengths, 0)
                roi_flux = np.where(mask, flux, 0)
                # roi_wavelengths = wavelengths[mask]
                # roi_flux = flux[mask]
                # å¦‚æžœroi_wavelengthsé•¿åº¦éž0
                if len(roi_wavelengths) == 0:
                    continue
                roi_peaks = _find_features_multiscale(
                    roi_wavelengths, roi_flux,
                    state, feature="peak", sigma_list=sigma_list,
                    prom=prom_peaks, tol_pixels=tol_pixels, weight_original=weight_original,
                    use_continuum_for_trough=True
                )
                roi_troughs = _find_features_multiscale(
                    roi_wavelengths, roi_flux,
                    state, feature="trough", sigma_list=sigma_list,
                    prom=prom_troughs, tol_pixels=tol_pixels, weight_original=weight_original,
                    use_continuum_for_trough=True,
                    min_depth=0.08
                )
                ROI_peaks.extend(roi_peaks)
                ROI_troughs.extend(roi_troughs)
            roi_edges_ = roi_edges + 250
            for i in range(len(roi_edges_)-1):
                roi_start = roi_edges_[i]
                roi_end = roi_edges_[i+1]
                mask = (wavelengths >= roi_start) & (wavelengths < roi_end)
                # roi_wavelengthsé•¿åº¦ä¸Žwavelengthsç›¸åŒï¼Œmaskä¹‹å¤–çš„ä½ç½®ä¸º0ï¼Œmaskå†…çš„ä½ç½®ä¸ºåŽŸå§‹å€¼
                roi_wavelengths = np.where(mask, wavelengths, 0)
                roi_flux = np.where(mask, flux, 0)
                # roi_wavelengths = wavelengths[mask]
                # roi_flux = flux[mask]
                # å¦‚æžœroi_wavelengthsé•¿åº¦éž0
                if len(roi_wavelengths) == 0:
                    continue
                roi_peaks = _find_features_multiscale(
                    roi_wavelengths, roi_flux,
                    state, feature="peak", sigma_list=sigma_list,
                    prom=prom_peaks, tol_pixels=tol_pixels, weight_original=weight_original,
                    use_continuum_for_trough=True
                )
                roi_troughs = _find_features_multiscale(
                    roi_wavelengths, roi_flux,
                    state, feature="trough", sigma_list=sigma_list,
                    prom=prom_troughs, tol_pixels=tol_pixels, weight_original=weight_original,
                    use_continuum_for_trough=True,
                    min_depth=0.08
                )
                ROI_peaks.extend(roi_peaks)
                ROI_troughs.extend(roi_troughs)
            state["ROI_peaks"] = ROI_peaks
            state["ROI_troughs"] = ROI_troughs
            state['merged_peaks'], state['merged_troughs'] = merge_features(
                wavelengths, flux,
                global_peaks=state["peaks"],
                global_troughs=state["troughs"],
                ROI_peaks=state["ROI_peaks"],
                ROI_troughs=state["ROI_troughs"],
                tol_pixels=tol_pixels
            )
        except Exception as e:
            print(f"âŒ peak_trough_detection: {e}")
        return state

    async def continuum_fitting(self, state: SpectroState):
        """ç®€å•çš„continuumæ‹Ÿåˆ"""
        try:
            spec = state["spectrum"]
            wavelengths = np.array(spec["new_wavelength"])
            flux = np.array(spec["weighted_flux"])

            band_name = state['band_name']
            band_wavelength = state['band_wavelength']
            print('cut continuum')
            if band_name:
                overlap_regions = find_overlap_regions(band_name, band_wavelength)
                # åˆå§‹åŒ– mask ä¸ºå…¨ False
                mask = np.zeros_like(wavelengths, dtype=bool)
                for key in overlap_regions:
                    low, high = overlap_regions[key]
                    region_mask = (wavelengths >= low) & (wavelengths <= high)
                    mask = mask | region_mask  # æˆ–è€…ç”¨ mask |= region_mask
                wavelengths = wavelengths[~mask]
                flux = flux[~mask]

            sigma_contunuum = getenv_int('CONTINUUM_SMOOTHING_SIGMA', None)
            print(f'CONTINUUM_SMOOTHING_SIGMA: {sigma_contunuum}')
            if sigma_contunuum == None:
                logging.error("CONTINUUM_SMOOTHING_SIGMA æœªè®¾ç½®")
                return
            continuum_flux = gaussian_filter1d(flux, sigma=sigma_contunuum)
            state['continuum'] = {
                'wavelength': wavelengths.tolist(),
                'flux': continuum_flux.tolist()
            }
        except Exception as e:
            print(f"âŒ continuum_fitting: {e}")
        return state

    # --------------------------
    # Step 1.1~1.11: ä¸»æµç¨‹
    # --------------------------
    async def run(self, state: SpectroState, plot: bool = True):
        """æ‰§è¡Œå®Œæ•´è§†è§‰åˆ†æžæµç¨‹"""
        try:
            # Step 1.1: è§†è§‰ LLM æå–åæ ‡è½´
            await self.detect_axis_ticks(state)
            # Step 1.2: OCR æå–åˆ»åº¦
            state["OCR_detected_ticks"] = _detect_axis_ticks(state['image_path'])
            print(state["OCR_detected_ticks"])
            # Step 1.3: åˆå¹¶
            await self.combine_axis_mapping(state)
            # Step 1.4: ä¿®æ­£
            await self.revise_axis_mapping(state)
            # Step 1.5: è¾¹æ¡†æ£€æµ‹ä¸Žè£å‰ª
            state['margin'] = {
                'top': 20,
                'right': 10,
                'bottom': 15,
                'left': 10,
            }
            stop = False
            while stop is False:
                state["chart_border"] = _detect_chart_border(state['image_path'], state['margin'])
                _crop_img(state['image_path'], state["chart_border"], state['crop_path'])
                box_new = await self.check_border(state)
                values = [box_new['top'], box_new['bottom'], box_new['left'], box_new['right']]
                margin = [state['margin']['top'], state['margin']['right'], state['margin']['bottom'], state['margin']['left']] 
                if all(values):  # æ‰€æœ‰éƒ½æ˜¯ Trueï¼ˆéžé›¶/éžFalseï¼‰
                    stop = True
                elif any(m > 30 for m in margin):
                    stop = True
                else:
                    for k, v in box_new.items():
                        if v == True:
                            state['margin'][k] = state['margin'][k]
                        else:
                            state['margin'][k] = state['margin'][k] + 2
                print(f"box_new: {box_new}")
                print(f"margin: {state['margin']}")
            # Step 1.6: é‡æ˜ å°„åƒç´ 
            state["tick_pixel_remap"] = _remap_to_cropped_canvas(state['tick_pixel_raw'], state["chart_border"])
            # Step 1.7: æ‹Ÿåˆåƒç´ -æ•°å€¼
            state["pixel_to_value"] = _pixel_tickvalue_fitting(state['tick_pixel_remap'])
            # Step 1.8: æå–æ›²çº¿ & ç°åº¦åŒ–
            curve_points, curve_gray_values = _process_and_extract_curve_points(state['crop_path'])
            state["curve_points"] = curve_points
            state["curve_gray_values"] = curve_gray_values
            # Step 1.9: å…‰è°±è¿˜åŽŸ
            state["spectrum"] = _convert_to_spectrum(state['curve_points'], state['curve_gray_values'], state['pixel_to_value'])
            # Step 1.10: æ£€æµ‹å³°å€¼/è°·å€¼
            await self.peak_trough_detection(state)
            print(f"Detected {len(state['merged_peaks'])} peaks and {len(state['merged_troughs'])} troughs.")
            # Step 1.10.5: continuumæ‹Ÿåˆ
            await self.continuum_fitting(state)
            # Step 1.11: å¯é€‰ç»˜å›¾
            if plot:
                try:
                    state["spectrum_fig"] = _plot_spectrum(state)
                except Exception as e:
                    print(f"âŒ plot spectrum or features terminated with error: {e}")
                    raise
            return state
        except Exception as e:
            print(f"âŒ run pipeline terminated with error: {e}")
            raise

# ---------------------------------------------------------
# 2. Rule-based Analyst â€” è´Ÿè´£åŸºäºŽè§„åˆ™çš„ç‰©ç†åˆ†æž
# ---------------------------------------------------------
class SpectralRuleAnalyst(BaseAgent):
    """
    è§„åˆ™é©±åŠ¨åž‹åˆ†æžå¸ˆï¼šåŸºäºŽç»™å®šçš„ç‰©ç†ä¸Žè°±çº¿çŸ¥è¯†è¿›è¡Œå®šæ€§åˆ†æž
    """

    def __init__(self, mcp_manager: MCPManager):
        super().__init__(
            agent_name='Spectral Rule Analyst',
            mcp_manager=mcp_manager
        )

    async def describe_spectrum_picture(self, state: SpectroState):
        function_prompt = state['prompt'][f'{self.agent_name}']['describe_spectrum_picture']
        async def _filter_noise(state):
            band_name = state['band_name']
            band_wavelength = state['band_wavelength']

            if not band_name or not band_wavelength:
                return {
                    "filter_noise": 'false',
                    "filter_noise_wavelength": None
                }
            else:
                # æ‰¾å‡ºé‡å åŒºåŸŸ
                overlap_regions = find_overlap_regions(band_name, band_wavelength)
                spec = state['spectrum']
                wl = np.array(spec['new_wavelength'])
                d_f = np.array(spec['delta_flux'])

                system_prompt = function_prompt['_filter_noise']['system_prompt']
                band_name_json = json.dumps(band_name, ensure_ascii=False)
                ham = f"""
æœ¬å…‰è°±çš„ camera/filters åä¸º
{band_name_json}
ä¸‹é¢æ˜¯å…‰è°±åœ¨ camera/filters äº¤ç•ŒåŒºåŸŸçš„æ ·æœ¬æ•°æ®ã€‚
"""
                for key in overlap_regions.keys():
                    overlap = overlap_regions[key]
                    scale = overlap[1] - overlap[0]
                    scale = scale * 2
                    center = (overlap[0] + overlap[1]) / 2
                    left = center - scale / 2
                    right = center + scale / 2
                    mask = (wl >= left) & (wl <= right)
                    wl_t = wl[mask]
                    wl_t = wl_t.tolist()
                    wl_t_json = json.dumps(wl_t, ensure_ascii=False)
                    delta_t = d_f[mask]
                    delta_t = delta_t.tolist()
                    delta_t_json = json.dumps(delta_t, ensure_ascii=False)

                    ham += f"""
äº¤ç•ŒåŒºåŸŸ {key}:
æ³¢é•¿ï¼š{wl_t_json}
Flux è¯¯å·®ï¼š{delta_t_json}
"""
                user_prompt = function_prompt['_filter_noise']['user_prompt']
                user_prompt = ham + user_prompt

                response = await self.call_llm_with_context(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    image_path=None,
                    parse_json=True,
                    description="Filterå™ªå£°åˆ¤æ–­"
                )
                return(response)
            
        async def _cleaning(state):
            filter_nosie = state['visual_interpretation'][0]
            if not safe_to_bool(filter_nosie.get('filter_noise', False)):
                state['cleaned_peaks'] = state['merged_peaks']
                state['cleaned_troughs'] = state['merged_troughs']
            else:
                filter_noise_wl = filter_nosie.get('filter_noise_wavelength', [])
                filter_noise_wl = np.array(filter_noise_wl)
                wavelength = np.array(state['spectrum']['new_wavelength'])
                peaks = state['merged_peaks']
                cleaned_peaks = []
                wiped_peaks = []
                for p in peaks:
                    wl = p['wavelength']
                    width = p['width_mean']

                    distance = abs(wl - filter_noise_wl)
                    # å¦‚æžœåœ¨distanceä¸­è‡³å°‘æœ‰ä¸€ä¸ªå€¼å°äºŽ widthï¼Œåˆ™è®¤ä¸ºè¯¥å³°åœ¨å™ªå£°åŒºåŸŸå†…
                    if np.any(distance <= width):
                        is_artifact = True
                    else:
                        is_artifact = False
                    if not is_artifact:
                        if p['width_in_km_s'] is not None and p['wavelength'] > wavelength[0]:
                            if p['width_in_km_s'] > 2000:
                                p['describe'] = 'å®½çº¿'
                            elif p['width_in_km_s'] < 1000:
                                p['describe'] = 'çª„çº¿'
                            else:
                                p['describe'] = 'ä¸­ç­‰å®½åº¦'
                            cleaned_peaks.append(p)
                    else:
                        wiped_peaks.append(p)
                state['cleaned_peaks'] = cleaned_peaks
                state['wiped_peaks'] = wiped_peaks

                cleaned_troughs = []
                for t in state['merged_troughs']:
                    wl = t['wavelength']
                    distance = abs(wl - filter_noise_wl)
                    if np.any(distance <= width):
                        is_artifact = True
                    else:
                        is_artifact = False
                    if not is_artifact:
                        if t['width_in_km_s'] is not None and t['wavelength'] > wavelength[0]:
                            if t['width_in_km_s'] > 2000:
                                t['describe'] = 'å®½è°·'
                            elif t['width_in_km_s'] < 1000:
                                t['describe'] = 'çª„è°·'
                            else:
                                t['describe'] = 'ä¸­ç­‰å®½åº¦'
                        else:
                            t['describe'] = 'æœªå¤„ç†'
                        cleaned_troughs.append(t)
                state['cleaned_troughs'] = cleaned_troughs
            return state

        async def _visual(state):
            system_prompt = function_prompt['_visual']['system_prompt']
            user_prompt_1 = function_prompt['_visual']['user_prompt_continuum']
            response_1 = await self.call_llm_with_context(
                system_prompt=system_prompt,
                user_prompt=user_prompt_1,
                image_path=state['continuum_path'],
                parse_json=True,
                description="è§†è§‰å…‰è°±å®šæ€§æè¿°â€”â€”continuum"
            )
            
            user_prompt_2 = function_prompt['_visual']['user_prompt_lines']
            response_2 = await self.call_llm_with_context(
                system_prompt=system_prompt,
                user_prompt=user_prompt_2,
                image_path=state['spec_extract_path'],
                parse_json=True,
                description="è§†è§‰å…‰è°±å®šæ€§æè¿°"
            )

            user_prompt_3 = function_prompt['_visual']['user_prompt_quality']
            response_3 = await self.call_llm_with_context(
                system_prompt=system_prompt,
                user_prompt=user_prompt_3,
                image_path=state['spec_extract_path'],
                parse_json=True,
                description="è§†è§‰å…‰è°±å®šæ€§æè¿°"
            )

            response_1_json = json.dumps(response_1, ensure_ascii=False)
            response_2_json = json.dumps(response_2, ensure_ascii=False)
            response_3_json = json.dumps(response_3, ensure_ascii=False)
            return '\n'.join([response_1_json, response_2_json, response_3_json])

        async def _integrate(state):
            visual_json       = json.dumps(state['visual_interpretation'][1], ensure_ascii=False)

            system_prompt = function_prompt['_integrate']['system_prompt']
            ham = f"""
{visual_json}
"""
            user_prompt_integrate = function_prompt['_integrate']['user_prompt'] + ham
            response = await self.call_llm_with_context(
                system_prompt=system_prompt,
                user_prompt=user_prompt_integrate,
                parse_json=True,
                description="è§†è§‰å…‰è°±å®šæ€§æè¿°"
            )
            return response

        result_filter_noise = await _filter_noise(state)
        state['visual_interpretation'] = [result_filter_noise]
        await _cleaning(state)
        result_visual = await _visual(state)
        state['visual_interpretation'].append(result_visual)
        result_integrate = await _integrate(state)
        state['visual_interpretation'] = result_integrate

        visual_interpretation_path = os.path.join(state['output_dir'], f'{state['image_name']}_visual_interpretation.txt')
        with open(visual_interpretation_path, 'w', encoding='utf-8') as f:
            json_str = json.dumps(state['visual_interpretation'], indent=2, ensure_ascii=False)
            f.write(json_str)
        print('finished describe_spectrum_picture')
    
    async def preliminary_classification(self, state: SpectroState) -> str:
        """åˆæ­¥åˆ†ç±»ï¼šæ ¹æ®å…‰è°±å½¢æ€åˆæ­¥åˆ¤æ–­å¤©ä½“ç±»åž‹"""

        continuum_interpretation_json = json.dumps(state['visual_interpretation']['continuum_description'], ensure_ascii=False)
        line_interpretation_json = json.dumps(state['visual_interpretation']['lines_description'], ensure_ascii=False)

        system_prompt = """
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å¤©æ–‡å­¦å…‰è°±åˆ†æžåŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å…‰è°±çš„å®šæ€§æè¿°å’Œç‰¹å¾æ•°æ®ï¼ŒçŒœæµ‹å¤©ä½“å¯èƒ½å±žäºŽçš„ç±»åˆ«ã€‚

å¦‚æžœè¿žç»­è°±å‘ˆçŽ°è“ç«¯è¾ƒé«˜ï¼Œçº¢ç«¯è¾ƒä½Žçš„è¶‹åŠ¿ï¼ˆå³é«˜â†’ä½Žï¼‰ï¼Œåˆ™è¯¥å¤©ä½“ä¸º QSOï¼›
å¦‚æžœè¿žç»­è°±å‘ˆçŽ°è“ç«¯è¾ƒä½Žï¼Œä¸­æ®µè¾ƒé«˜ï¼Œçº¢ç«¯ä¸‹é™çš„è¶‹åŠ¿ï¼ˆå³ä½Žâ†’é«˜â†’ä½Žï¼‰ï¼Œåˆ™è¯¥å¤©ä½“ä¸º QSO ï¼›

å¦‚æžœè¿žç»­è°±å‘ˆçŽ°è“ç«¯è¾ƒä½Žï¼Œçº¢ç«¯è¾ƒé«˜çš„è¶‹åŠ¿ï¼ˆå³ä½Žâ†’é«˜ï¼‰ï¼Œåˆ™è¯¥å¤©ä½“ä¸º Galaxy ï¼›

æ¯”è¾ƒä¸¤ç§å…‰æºçš„æ¦‚çŽ‡ï¼Œç»™å‡ºä½ çš„é€‰æ‹©ã€‚

è¾“å‡ºå¤©ä½“ç±»åˆ«ï¼Œæ ¼å¼ä¸º jsonï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
    'type': str,  # å¤©ä½“ç±»åˆ«ï¼Œå¯èƒ½çš„å–å€¼ä¸º "Galaxy", "QSO"
}

ä»…è¾“å‡ºå”¯ä¸€é€‰é¡¹ã€‚ä¸è¦è¾“å‡ºå…¶ä»–ä¿¡æ¯ã€‚
"""
        user_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹å…‰è°±æ•°æ®è¿›è¡Œåˆ†æžï¼š

å‰ä¸€ä½å¤©æ–‡å­¦åŠ©æ‰‹å·²ç»å®šæ€§åœ°æè¿°äº†å…‰è°±çš„æ•´ä½“å½¢æ€ï¼š
{continuum_interpretation_json}

è¯·æ ¹æ®æè¿°å’Œå›¾åƒï¼ŒçŒœæµ‹è¯¥å…‰è°±å¯èƒ½å±žäºŽå“ªä¸€ç±»å¤©ä½“ã€‚
"""+"""
è¾“å‡ºä¸º jsonï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
    'type': str,  # å¤©ä½“ç±»åˆ«ï¼Œå¯èƒ½çš„å–å€¼ä¸º "Galaxy", "QSO"
}
"""
#         user_prompt = f"""
# è¯·æ ¹æ®å›¾åƒï¼ŒçŒœæµ‹è¯¥å…‰è°±å¯èƒ½å±žäºŽå“ªä¸€ç±»å¤©ä½“ã€‚
# """
        response = await self.call_llm_with_context(
            system_prompt = system_prompt,
            user_prompt = user_prompt,
            image_path=None,
            parse_json=True,
            description="åˆæ­¥åˆ†ç±»"
        )
        state['preliminary_classification'] = response

    async def preliminary_classification_monkey(self, state):
        """ My dear monkey friend and its typewriter """
        preliminary_classification_json = json.dumps(state['preliminary_classification'], ensure_ascii=False)
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå¤©æ–‡å­¦å…‰è°±åˆ†æžåŠ©æ‰‹ã€‚
ä½ æŽ¥æ”¶åˆ°çš„æ˜¯å…¶ä»–åŠ©æ‰‹å¯¹ä¸€å¼ å…‰è°±çš„å…‰æºç±»åˆ«çš„åˆæ­¥çŒœæµ‹ï¼š
{preliminary_classification_json}

è¯·è¾“å‡ºè¿™ä»½çŒœæµ‹é‡Œç»™å‡ºçš„å…‰æºç±»åˆ«ã€‚
"""+"""
æ ¼å¼ä¸º jsonï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
    'type': str,  # å¤©ä½“ç±»åˆ«ï¼Œå¯èƒ½çš„å–å€¼ä¸º "Galaxy", "QSO"
}
ä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹
"""
        response = await self.call_llm_with_context(
            system_prompt = '',
            user_prompt = prompt,
            parse_json=True,
            description="åˆæ­¥åˆ†ç±»çŒ´å­"
        )
        return response
    ###################################
    # QSO part
    ###################################
    async def _QSO(self, state):
        """QSO"""
        try:
            peaks_info = [
                {
                    "wavelength": pe.get('wavelength'),
                    "flux": pe.get('mean_flux'),
                    "width": pe.get('width_mean'),
                    "width_in_km_s": pe.get('width_in_km_s'),
                    "prominance": pe.get('max_prominence'),
                    "seen_in_max_global_smoothing_scale_sigma": pe.get('max_global_sigma_seen', None),
                    "seen_in_max_local_smoothing_scale_sigma": pe.get('max_roi_sigma_seen', None),
                    "describe": pe.get('describe')
                }
                for pe in state.get('cleaned_peaks', [])[:15]
            ]
            peak_json = json.dumps(peaks_info, ensure_ascii=False)

            # åˆå§‹åŒ–LyÎ±å€™é€‰çº¿åˆ—è¡¨
            Lyalpha_candidate = []

            # èŽ·å–å…‰è°±æ³¢é•¿èŒƒå›´
            wl_left = state['spectrum']['new_wavelength'][0]
            wl_right = state['spectrum']['new_wavelength'][-1]
            mid_wavelength = (wl_left + wl_right) / 2

            # ç­›é€‰æ¡ä»¶1ï¼šä¼˜å…ˆä½¿ç”¨å…¨å±€å¹³æ»‘å°ºåº¦çš„ä¿¡å™ªæ¯”
            for peak in peaks_info:
                # æ£€æŸ¥è°±çº¿å®½åº¦æ˜¯å¦è¶³å¤Ÿï¼ˆ>=2000 km/sï¼‰
                if peak['width_in_km_s'] is not None and peak['width_in_km_s'] >= 2000:
                    # æ£€æŸ¥å…¨å±€å¹³æ»‘å°ºåº¦çš„ä¿¡å™ªæ¯”æ¡ä»¶
                    if (peak['seen_in_max_global_smoothing_scale_sigma'] is not None and 
                        peak['seen_in_max_global_smoothing_scale_sigma'] > 2):
                        Lyalpha_candidate.append(peak['wavelength'])

            # ç­›é€‰æ¡ä»¶2ï¼šå¦‚æžœæ¡ä»¶1æ²¡æœ‰æ‰¾åˆ°å€™é€‰ï¼Œä½¿ç”¨å±€éƒ¨å¹³æ»‘å°ºåº¦çš„ä¿¡å™ªæ¯”
            if len(Lyalpha_candidate) == 0:
                for peak in peaks_info:
                    if peak['width_in_km_s'] is not None and peak['width_in_km_s'] >= 2000:
                        # æ£€æŸ¥å±€éƒ¨å¹³æ»‘å°ºåº¦çš„ä¿¡å™ªæ¯”æ¡ä»¶
                        if (peak['seen_in_max_local_smoothing_scale_sigma'] is not None and 
                            peak['seen_in_max_local_smoothing_scale_sigma'] > 2):
                            Lyalpha_candidate.append(peak['wavelength'])

            # å°†å€™é€‰çº¿è½¬æ¢ä¸ºJSONæ ¼å¼å¹¶æ‰“å°
            Lyalpha_candidate_json = json.dumps(Lyalpha_candidate, ensure_ascii=False)
            print(f"Lyalpha_candidate: {Lyalpha_candidate}")

            trough_info = [
                {
                    "wavelength": tr.get('wavelength'),
                    "flux": tr.get('mean_flux'),
                    "width": tr.get('width_mean'),
                    "seen_in_scales_of_sigma": tr.get('seen_in_scales_of_sigma')
                }
                for tr in state.get('cleaned_troughs', [])[:15]
            ]
            trough_json = json.dumps(trough_info, ensure_ascii=False)
            # print(f"trough_info: {trough_info}")
        except Exception as e:
            logging.error(f"Error in _QSO: {e}")
            raise e

        def _common_prompt_header_QSO(state, include_rule_analysis=True, include_step_1_only=False):
            """æž„é€ æ¯ä¸ª step å…¬å…±çš„ prompt å‰æ®µ"""
            try:
                visual_json = json.dumps(state['visual_interpretation'], ensure_ascii=False)
                # peak_json = json.dumps(state['peaks'][:10], ensure_ascii=False)
                # trough_json = json.dumps(state['troughs'], ensure_ascii=False)
                header = f"""
ä½ æ˜¯ä¸€ä½å¤©æ–‡å­¦å…‰è°±åˆ†æžåŠ©æ‰‹ã€‚

ä»¥ä¸‹ä¿¡æ¯å¯èƒ½æ¥è‡ªäºŽä¸€ä¸ªæœªçŸ¥çº¢ç§»çš„ QSO å…‰è°±ã€‚

ä¹‹å‰çš„åŠ©æ‰‹å·²ç»å¯¹è¿™ä¸ªå…‰è°±è¿›è¡Œäº†åˆæ­¥æè¿°ï¼š
{visual_json}

è¯¥å…‰è°±çš„æ³¢é•¿èŒƒå›´æ˜¯{state['spectrum']['new_wavelength'][0]} Ã… åˆ° {state['spectrum']['new_wavelength'][-1]} Ã…ã€‚
"""

                if include_rule_analysis and state['rule_analysis_QSO']:
                    if include_step_1_only==True:
                        rule_json = json.dumps(state['rule_analysis_QSO'][0], ensure_ascii=False)
                    else:
                        rule_json = json.dumps("\n".join(str(item) for item in state['rule_analysis_QSO']), ensure_ascii=False)
                    header += f"\nä¹‹å‰çš„åŠ©æ‰‹å·²ç»è¿›è¡Œäº†ä¸€äº›åˆ†æž:\n{rule_json}\n"

                tol_pixels = getenv_int("TOL_PIXELS", 10)
                a_x = state['pixel_to_value']['x']['a']
                tol_wavelength = a_x * tol_pixels
                header += f"""
ç»¼åˆåŽŸæ›²çº¿å’Œ smoothing å°ºåº¦ä¸º sigma={state['sigma_list']} çš„é«˜æ–¯å¹³æ»‘æ›²çº¿ï¼Œä½¿ç”¨ scipy å‡½æ•°è¿›è¡Œäº†å³°/è°·è¯†åˆ«ã€‚
å…³äºŽå³°/è°·çš„è®¨è®ºä»¥ä»¥ä¸‹æ•°æ®ä¸ºå‡†ï¼š
- ä»£è¡¨æ€§çš„å‰ 10 æ¡å‘å°„çº¿ï¼š
{peak_json}
- å¯èƒ½çš„å¸æ”¶çº¿ï¼š
{trough_json}
- æ³¢é•¿è¯¯å·®åœ¨ ~ Â±{tol_wavelength/2} Ã… çš„é‡çº§æˆ–æ›´å¤§
"""
                return header
            except Exception as e:
                logging.error(f"Error in _common_prompt_header_QSO: {e}")
                raise e

        def _common_prompt_tail(step_title, extra_notes=""):
            """æž„é€ æ¯ä¸ª step å…¬å…±å°¾éƒ¨ï¼Œä¿ç•™ step ç‰¹æœ‰è¾“å‡º/åˆ†æžæŒ‡ç¤º"""
            try:
                tail = f"""
---

è¾“å‡ºæ ¼å¼ä¸ºï¼š
{step_title}
...

---

ðŸ§­ æ³¨æ„ï¼š
- è®¡ç®—å¾—æ¥çš„éžåŽŸå§‹æ•°æ®ï¼Œè¾“å‡ºæ—¶ä¿ç•™ 3 ä½å°æ•°ã€‚
- ä¸éœ€è¦è¿›è¡Œé‡å¤æ€»ç»“ã€‚
- ä¸éœ€è¦é€è¡Œåœ°é‡å¤è¾“å…¥æ•°æ®ï¼›
- é‡ç‚¹åœ¨ç‰©ç†æŽ¨ç†ä¸Žåˆç†è§£é‡Šï¼›
- è¯·ä¿è¯æœ€ç»ˆè¾“å‡ºå®Œæ•´ï¼Œä¸è¦ä¸­é€”æˆªæ–­ã€‚
"""
                if extra_notes:
                    tail = extra_notes + "\n" + tail
                return tail
            except Exception as e:
                logging.error(f"Error in _common_prompt_tail: {e}")
                raise e
        
        async def step_1_QSO(state):
            try:
                print("Step 1: LyÎ± è°±çº¿æ£€æµ‹")
                header = _common_prompt_header_QSO(state, include_rule_analysis=False)
                tail = _common_prompt_tail("Step 1: LyÎ± è°±çº¿æ£€æµ‹")
                if len(Lyalpha_candidate) > 0:
                    candidate_str = f"\nç®—æ³•ç­›é€‰çš„ LyÎ± å€™é€‰çº¿åŒ…æ‹¬ï¼š\n{Lyalpha_candidate_json}\nä½ ä¹Ÿå¯ä»¥è‡ªå·±æŽ¨æµ‹å…¶ä»–é€‰é¡¹ã€‚\n"
                else:
                    candidate_str = ""

                system_prompt = header + tail
                user_prompt = f"""
è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æž:

Step 1: LyÎ± è°±çº¿æ£€æµ‹
å‡è®¾è¯¥å…‰è°±ä¸­å­˜åœ¨ LyÎ± å‘å°„çº¿ï¼ˆÎ»_rest = 1216 Ã…ï¼‰ï¼š
{candidate_str}
1. åœ¨å…‰è°±æµé‡è¾ƒå¤§ï¼Œå¤§ smoothing å°ºåº¦å¯è§ä¸”æœ‰ä¸€å®šå®½åº¦çš„å³°ä¸­ï¼ŒæŽ¨æµ‹å“ªæ¡æœ€å¯èƒ½ä¸º LyÎ± çº¿ã€‚
    - ä»Žæä¾›çš„å³°åˆ—è¡¨ä¸­é€‰æ‹©
    - å€™é€‰è°±çº¿å®½åº¦ç›¸è¿‘ï¼ˆ20 Ã… ä»¥å†…ï¼‰æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘æµé‡æ›´é«˜çš„å³°ã€‚
2. è¾“å‡ºï¼š
- è§‚æµ‹æ³¢é•¿ Î»_obs
- æµé‡ Flux
- è°±çº¿å®½åº¦
3. ä½¿ç”¨å·¥å…· calculate_redshift è®¡ç®—è¯¥å³°ä¸º LyÎ± å‘å°„çº¿æ—¶çš„çº¢ç§» zã€‚
4. æ£€æŸ¥è“ç«¯ï¼ˆçŸ­æ³¢é•¿æ–¹å‘ï¼‰æ˜¯å¦å­˜åœ¨ LyÎ± forest ç‰¹å¾ï¼šå¸æ”¶çº¿ç›¸å¯¹æ›´å¯†é›†ã€è¾ƒçª„ä¸”åˆ†å¸ƒåœ¨ LyÎ± è“ç«¯é™„è¿‘ã€‚è¯·æŒ‡å‡ºå¹¶è¿›è¡Œç®€çŸ­è¯´æ˜Žã€‚
""" 
                
                response = await self.call_llm_with_context(
                    system_prompt=system_prompt, 
                    user_prompt=user_prompt, 
                    parse_json=True, 
                    description="Step 1 LyÎ± åˆ†æž"
                )
                state['rule_analysis_QSO'].append(response)
            except Exception as e:
                logging.error(f"Error in step_1_QSO: {e}")
                raise e

        async def step_2_QSO(state):
            print("Step 2: å…¶ä»–æ˜¾è‘—å‘å°„çº¿åˆ†æž")
            try:
                header = _common_prompt_header_QSO(state)
                tail = _common_prompt_tail("Step 2: å…¶ä»–æ˜¾è‘—å‘å°„çº¿åˆ†æž")
                system_prompt = header + tail

                band_name = state['band_name']
                band_wavelength = state['band_wavelength']
                if band_name: 
                    overlap_regions = find_overlap_regions(band_name, band_wavelength)
                    wws = np.max([wp.get('width_mean') for wp in state.get('wiped_peaks', [])[:5]])
                    print(f"wws: {wws}")
                    for key in overlap_regions:
                        range = overlap_regions[key]
                        overlap_regions[key] = [range[0]-wws, range[1]+wws] # Broaden the overlap regions to make sure LLM won't miss them
                    overlap_regions_json = json.dumps(overlap_regions, ensure_ascii=False)
                    wiped = [
                        {
                            "wavelength": wp.get('wavelength'),
                            "flux": wp.get('mean_flux'),
                            "width": wp.get('width_mean'),
                            # "seen_in_scales_of_sigma": wp.get('seen_in_scales_of_sigma')
                        }
                        for wp in state.get('wiped_peaks', [])[:5]
                    ]
                    wiped_json = json.dumps(wiped, ensure_ascii=False)
                    advanced = f"""\n    - æ³¨æ„ï¼šå¦‚æžœæŸäº›ç†è®ºå³°å€¼è½åœ¨ä»¥ä¸‹åŒºé—´é™„è¿‘ï¼š\n        {overlap_regions_json}\n    åˆ™å³°å€¼å¯èƒ½è¢«å½“ä½œå™ªå£°ä¿¡å·æ¸…é™¤ã€‚è¿™äº›å³°å€¼æ˜¯ï¼š\n        {wiped_json}\n    è¯·ä¼˜å…ˆè€ƒè™‘è¿™äº›å› ç´ ï¼Œå†æ¬¡åˆ†æž"""
                else:
                    advanced = ""

                user_prompt = f"""
è¯·ç»§ç»­åˆ†æž:

Step 2: å…¶ä»–æ˜¾è‘—å‘å°„çº¿åˆ†æž
1. åœ¨ Step 1 å¾—åˆ°çš„çº¢ç§»ä¸‹ï¼Œä½¿ç”¨å·¥å…· predict_obs_wavelength è®¡ç®—ä»¥ä¸‹ä¸‰æ¡ä¸»è¦å‘å°„çº¿ï¼šC IV 1549, C III] 1909, Mg II 2799 åœ¨å…‰è°±ä¸­çš„ç†è®ºä½ç½®ã€‚
2. æç¤ºè¯æä¾›çš„å…‰è°±ä¸­æ˜¯å¦æœ‰ä¸Žä¸‰è€…ç›¸åŒ¹é…çš„å³°ï¼Ÿ{advanced}
3. å¦‚æžœå­˜åœ¨å‘å°„çº¿ä¸Žè§‚æµ‹å³°å€¼çš„åŒ¹é…ï¼Œæ ¹æ®åŒ¹é…ç»“æžœï¼Œåˆ†åˆ«ä½¿ç”¨å·¥å…· calculate_redshift è®¡ç®—çº¢ç§»ã€‚æŒ‰â€œå‘å°„çº¿å--é™æ­¢ç³»æ³¢é•¿--è§‚æµ‹æ³¢é•¿--çº¢ç§»â€çš„æ ¼å¼è¾“å‡ºã€‚
"""

                response = await self.call_llm_with_context(system_prompt, user_prompt, parse_json=False, description="Step 2 å‘å°„çº¿åˆ†æž")
                state['rule_analysis_QSO'].append(response)
            except Exception as e:
                logging.error(f"Error in step_2_QSO: {e}")
                raise e

        async def step_3_QSO(state):
            try:
                header = _common_prompt_header_QSO(state)
                tail = _common_prompt_tail("Step 3: ç»¼åˆåˆ¤æ–­")
                system_prompt = header + tail

                user_prompt = """
è¯·ç»§ç»­åˆ†æž:

Step 3: ç»¼åˆåˆ¤æ–­
1. åœ¨ Step 1 åˆ° Step 2 ä¸­ï¼Œå¦‚æžœï¼š
    - C IV å’Œ C III] ä¸¤æ¡ä¸»è¦è°±çº¿å­˜åœ¨ç¼ºå¤±æˆ–å¤§å¹…åç§»
    - ä½¿ç”¨ lyÎ± è°±çº¿è®¡ç®—çš„çº¢ç§»ä¸Žå…¶ä»–è°±çº¿çš„è®¡ç®—ç»“æžœä¸ä¸€è‡´ï¼Œ
æ­¤æ—¶è¯·è¾“å‡ºâ€œåº”ä¼˜å…ˆå‡è®¾ LyÎ± è°±çº¿æœªè¢«æ‰¾å³°ç¨‹åºæ•èŽ·â€ï¼Œå¹¶ç»“æŸ Step 3 çš„åˆ†æžã€‚ä¸è¦è¾“å‡ºå…¶ä»–ä¿¡æ¯ã€‚
2.ä»…åœ¨æœ‰æ˜¾è‘—çš„ LyÎ± å³°å€¼ï¼Œä¸”çº¢ç§»è®¡ç®—ç»“æžœä¸Žå…¶ä»–è°±çº¿åŸºæœ¬ä¸€è‡´æ—¶ï¼Œè¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
    - å› ä¸ºå¤©æ–‡å­¦ä¸­å­˜åœ¨å¤–æµç­‰çŽ°è±¡ï¼Œè¯·å°†å½“å‰æ‰€æœ‰åŒ¹é…ä¸­**æœ€ä½Žç”µç¦»æ€è°±çº¿çš„çº¢ç§»**ä½œä¸ºå…‰è°±çš„çº¢ç§»ã€‚è¾“å‡ºçº¢ç§»ç»“æžœã€‚ï¼ˆå› ä¸ºå­˜åœ¨ä¸å¯¹ç§°å’Œå±•å®½ï¼ŒLyÎ±çš„ç½®ä¿¡åº¦æ˜¯è¾ƒä½Žçš„ï¼‰
"""
                response = await self.call_llm_with_context(system_prompt, user_prompt, parse_json=False, description="Step 3 ç»¼åˆåˆ¤æ–­")
                state['rule_analysis_QSO'].append(response)
            except Exception as e:
                logging.error(f"Error in step_3_QSO: {e}")
                raise e
            
        async def step_4_QSO(state):
            try: 
                header = _common_prompt_header_QSO(state, include_step_1_only=True)
                tail = _common_prompt_tail("Step 4: è¡¥å……æ­¥éª¤ï¼ˆå‡è®¾ Step 1 æ‰€é€‰æ‹©çš„è°±çº¿å¹¶éž LyÎ±ï¼‰")
                system_prompt = header + tail

                user_prompt = """
è¯·ç»§ç»­åˆ†æž:

Step 4: è¡¥å……æ­¥éª¤ï¼ˆå‡è®¾ Step 1 æ‰€é€‰æ‹©çš„è°±çº¿å¹¶éž LyÎ±ï¼‰
- è¯·æŠ›å¼€å‰è¿°æ­¥éª¤çš„åˆ†æžå†…å®¹ã€‚è€ƒè™‘ Step 1 æ‰€é€‰æ‹©çš„è°±çº¿å®žé™…ä¸Šæ˜¯é™¤ LyÎ± å¤–çš„å…¶ä»–ä¸»è¦å‘å°„çº¿ã€‚
    - å‡è®¾è¯¥å³°å€¼å¯èƒ½å¯¹åº”çš„è°±çº¿ä¸º C IVï¼š
        - è¾“å‡ºè¯¥å³°å¯¹åº”è°±çº¿çš„ä¿¡æ¯ï¼š
            - è§‚æµ‹æ³¢é•¿ Î»_obs
            - æµé‡ Flux
            - è°±çº¿å®½åº¦
            - æ ¹æ® Î»_restï¼Œä½¿ç”¨å·¥å…· calculate_redshift åˆæ­¥è®¡ç®—çº¢ç§» z
        - ä½¿ç”¨å·¥å…· predict_obs_wavelength è®¡ç®—åœ¨æ­¤çº¢ç§»ä¸‹çš„å…¶ä»–ä¸»è¦å‘å°„çº¿ï¼ˆå¦‚ LyÎ± C III] å’Œ Mg IIï¼‰çš„ç†è®ºä½ç½®ã€‚å…‰è°±ä¸­æ˜¯å¦æœ‰ä¸Žå®ƒä»¬åŒ¹é…çš„å‘å°„çº¿ï¼Ÿ
        - å¦‚æžœ LyÎ± è°±çº¿åœ¨å…‰è°±èŒƒå›´å†…ï¼Œæ£€æŸ¥å…¶æ˜¯å¦å­˜åœ¨ï¼Ÿ
        - å¦‚æžœå­˜åœ¨å¯èƒ½çš„å‘å°„çº¿-è§‚æµ‹æ³¢é•¿åŒ¹é…ç»“æžœï¼Œä½¿ç”¨å·¥å…· calculate_redshift è®¡ç®—å®ƒä»¬çš„çº¢ç§»ã€‚æŒ‰ç…§â€œå‘å°„çº¿å--é™æ­¢ç³»æ³¢é•¿--è§‚æµ‹æ³¢é•¿--çº¢ç§»â€çš„æ ¼å¼è¿›è¡Œè¾“å‡º
    
    - è‹¥ä»¥ä¸Šå‡è®¾ä¸åˆç†ï¼Œåˆ™å‡è®¾è¯¥å³°å€¼å¯èƒ½å¯¹åº” C III] ç­‰å…¶ä»–ä¸»è¦è°±çº¿ï¼Œé‡å¤æŽ¨æ–­ã€‚å¦‚æžœå…¶ä»–è°±çº¿ï¼ˆå¦‚ LyÎ± C III] å’Œ Mg IIï¼‰åœ¨å…‰è°±èŒƒå›´å†…ï¼Œæ£€æŸ¥å…¶æ˜¯å¦å­˜åœ¨ï¼Ÿ

- æ³¨æ„ï¼šå…è®¸åœ¨ç”±äºŽå…‰è°±è¾¹ç¼˜çš„ä¿¡å·æ®‹ç¼ºæˆ–ä¿¡å™ªæ¯”ä¸ä½³å¯¼è‡´éƒ¨åˆ†å‘å°„çº¿ä¸å¯è§ã€‚
""" + tail

                response = await self.call_llm_with_context(system_prompt, user_prompt, parse_json=False, description="Step 4 è¡¥å……åˆ†æž")
                state['rule_analysis_QSO'].append(response)
            except Exception as e:
                logging.error(f"Error in step_4_QSO: {e}")
                raise e
        
        await step_1_QSO(state)
        await step_2_QSO(state)
        await step_3_QSO(state)
        await step_4_QSO(state)

#     # --------------------------
#     # Run å…¨æµç¨‹
#     # --------------------------
    async def run(self, state: SpectroState):
        """æ‰§è¡Œè§„åˆ™åˆ†æžå®Œæ•´æµç¨‹"""
        try:
            await self.describe_spectrum_picture(state)

            plot_cleaned_features(state)
            await self.preliminary_classification(state)
            print(state['preliminary_classification'])

            # _shakespear = await self.preliminary_classification_monkey(state)
            # state['possible_object'] = _shakespear
            # print(f"Monkeys types: {_shakespear}")

            if state['preliminary_classification']['type'] == "QSO":
                await self._QSO(state)
            return state
        except Exception as e:
            import traceback
            print("âŒ An error occurred during spectral analysis:")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            print("Full traceback:")
            traceback.print_exc()
            # å¯é€‰ï¼šè¿”å›žå½“å‰çŠ¶æ€æˆ–æŠ›å‡ºå¼‚å¸¸
            raise  # å¦‚æžœä½ å¸Œæœ›è°ƒç”¨è€…ä¹Ÿèƒ½æ•èŽ·è¯¥å¼‚å¸¸
           
# # ---------------------------------------------------------
# # 3. Revision Supervisor â€” è´Ÿè´£äº¤å‰å®¡æ ¸ä¸Žè¯„ä¼°
# # ---------------------------------------------------------
class SpectralAnalysisAuditor(BaseAgent):
    """å®¡æŸ¥åˆ†æžå¸ˆï¼šå®¡æŸ¥å¹¶æ ¡æ­£å…¶ä»–åˆ†æž agent çš„è¾“å‡º"""

    def __init__(self, mcp_manager: MCPManager):
        super().__init__(
            agent_name='Spectral Analysis Auditor',
            mcp_manager=mcp_manager
        )

    def _common_prompt_header(self, state: SpectroState) -> str:
        try:
            peaks_info = [
                {
                    "wavelength": pe.get('wavelength'),
                    "flux": pe.get('mean_flux'),
                    "width": pe.get('width_mean'),
                    "prominance": pe.get('max_prominence'),
                    "seen_in_scales_of_sigma": pe.get('seen_in_scales_of_sigma'),
                    "describe": pe.get('describe')
                }
                for pe in state.get('cleaned_peaks', [])[:15]
            ]
            peak_json = json.dumps(peaks_info, ensure_ascii=False)
            trough_info = [
                {
                    "wavelength": tr.get('wavelength'),
                    "flux": tr.get('mean_flux'),
                    "width": tr.get('width_mean'),
                    "seen_in_scales_of_sigma": tr.get('seen_in_scales_of_sigma'), 
                }
                for tr in state.get('cleaned_troughs', [])[:15]
            ]
            trough_json = json.dumps(trough_info, ensure_ascii=False)
            a = state["pixel_to_value"]["x"]["a"]
            rms = state["pixel_to_value"]["x"]["rms"]
            tolerence = getenv_int("TOL_PIXELS", 10)
            rule_analysis = "\n\n".join(str(item) for item in state['rule_analysis_QSO'])
            prompt_1 = f"""
ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„ã€å¤©æ–‡å­¦å…‰è°±æŠ¥å‘Šå®¡æŸ¥åˆ†æžå¸ˆã€‘ã€‚

ä»»åŠ¡ç›®æ ‡ï¼š
- å®¡æ ¸å…¶ä»–åˆ†æžå¸ˆçš„å…‰è°±åˆ†æžæŠ¥å‘Šæˆ–æƒ³æ³•
- è¯†åˆ«å…¶ä¸­çš„é€»è¾‘æ¼æ´žã€è®¡ç®—æ¼æ´žã€ä¸ä¸€è‡´æˆ–é”™è¯¯æŽ¨æ–­
- æå‡ºä¿®æ­£æ„è§æˆ–è¡¥å……åˆ†æžæ–¹å‘

å·¥ä½œåŽŸåˆ™ï¼š
- ä¿æŒå®¢è§‚ä¸Žæ‰¹åˆ¤æ€§æ€ç»´
- ä¸é‡å¤åŽŸåˆ†æžï¼ŒåªæŒ‡å‡ºé—®é¢˜ä¸Žæ”¹è¿›å»ºè®®
- è‹¥åŽŸæŠ¥å‘Šåˆç†ï¼Œåº”æ˜Žç¡®ç¡®è®¤å…¶æœ‰æ•ˆæ€§
- æ¶‰åŠçº¢ç§»å’Œå…‰è°±è§‚æµ‹æ³¢é•¿çš„è®¡ç®—å¿…é¡»ä½¿ç”¨å·¥å…· calculate_redshift å’Œ  predict_obs_wavelengthã€‚ä¸å…è®¸è‡ªè¡Œè®¡ç®—ã€‚

è¾“å‡ºè¦æ±‚ï¼š
- è¯·è¾“å‡ºè¯´æ˜Žæ€§çš„è¯­è¨€
- ç®€æ˜Žåˆ—å‡ºå®¡æŸ¥æ„è§ï¼ˆä¾‹å¦‚ï¼šâ€œç»“è®ºåæ—©â€ï¼Œâ€œè°±çº¿è§£é‡Šæ­£ç¡®â€ï¼‰
- å¯¹æ¯ä¸ªå‘çŽ°é™„ä¸Šæ”¹è¿›å»ºè®®
- æœ€åŽç»™å‡ºæ•´ä½“è¯„ä»·ï¼ˆå¯é /éƒ¨åˆ†å¯ä¿¡/ä¸å¯ä¿¡ï¼‰

å·²çŸ¥ï¼šç»¼åˆåŽŸæ›²çº¿å’Œ sigma=2ã€sigma=4ã€sigma=16 ä¸‰æ¡é«˜æ–¯å¹³æ»‘æ›²çº¿ï¼Œä½¿ç”¨ scipy å‡½æ•°è¿›è¡Œäº†å³°/è°·è¯†åˆ«ã€‚
å…³äºŽå³°/è°·çš„è®¨è®ºä»¥ä»¥ä¸‹æ•°æ®ä¸ºå‡†ï¼š
- ä»£è¡¨æ€§çš„å‰ 10 æ¡å‘å°„çº¿ï¼š
{peak_json}
- å¯èƒ½çš„å¸æ”¶çº¿ï¼š
{trough_json}

å…¶ä»–åˆ†æžå¸ˆç»™å‡ºçš„å…‰è°±åˆ†æžæŠ¥å‘Šä¸ºï¼š

{rule_analysis}

è¯¥æŠ¥å‘Šåœ¨çº¢ç§»è®¡ç®—æ—¶ä¿ç•™äº† 3 ä½å°æ•°ã€‚

è¯¥å…‰è°±çš„æ³¢é•¿èŒƒå›´æ˜¯{state['spectrum']['new_wavelength'][0]} Ã… åˆ° {state['spectrum']['new_wavelength'][-1]} Ã…ã€‚
"""
            band_name = state['band_name']
            band_wavelength = state['band_wavelength']
            if band_name: 
                overlap_regions = find_overlap_regions(band_name, band_wavelength)
                wws = np.max([wp.get('width_mean') for wp in state.get('wiped_peaks', [])[:5]])
                for key in overlap_regions:
                    range = overlap_regions[key]
                    overlap_regions[key] = [range[0]-wws, range[1]+wws] # Broaden the overlap regions to make sure LLM won't miss them
                overlap_regions_json = json.dumps(overlap_regions, ensure_ascii=False)
                wiped = [
                    {
                        "wavelength": wp.get('wavelength'),
                        "flux": wp.get('mean_flux'),
                        "width": wp.get('width_mean'),
                        # "seen_in_scales_of_sigma": wp.get('seen_in_scales_of_sigma')
                    }
                    for wp in state.get('wiped_peaks', [])[:5]
                ]
                wiped_json = json.dumps(wiped, ensure_ascii=False)
                advanced = f"""å¦‚æžœæŠ¥å‘Šä¸­çš„å³°å€¼è½åœ¨ä»¥ä¸‹åŒºé—´é™„è¿‘\n    {overlap_regions_json}\nåˆ™å³°å€¼å¯èƒ½è¢«å½“ä½œå™ªå£°ä¿¡å·æ¸…é™¤ã€‚è¿™äº›å³°å€¼æ˜¯ï¼š\n      {wiped_json}\nè¯·æ³¨æ„è€ƒå¯Ÿè¿™äº›å³°å€¼ä½œä¸º C IV æˆ– C III] çš„å¯èƒ½æ€§"""
            else:
                advanced = ""
            prompt_2 = f"""

æˆ‘å¸Œæœ›å…‰è°±åˆ†æžæŠ¥å‘Šèƒ½å¤Ÿå°½å¯èƒ½å¥½åœ°åŒ¹é… LyÎ±ã€C IVã€C III]ã€Mg II ç­‰å…¸åž‹å‘å°„çº¿ï¼Œä½†ä¹Ÿå…è®¸åœ¨ç”±äºŽå…‰è°±è¾¹ç¼˜çš„ä¿¡å·æ®‹ç¼ºæˆ–ä¿¡å™ªæ¯”ä¸ä½³å¯¼è‡´éƒ¨åˆ†å‘å°„çº¿ä¸å¯è§ã€‚

åŒæ—¶ï¼Œåœ¨ä¿¡å™ªæ¯”ä¸ä½³æ—¶ï¼Œå¯»æ‰¾è°±çº¿çš„ç®—æ³•ä¹Ÿä¼šå—åˆ°å½±å“ï¼Œå› æ­¤ä¹Ÿå…è®¸çº¿å®½ä¸ŽæœŸæœ›å­˜åœ¨ä¸€å®šçš„çš„å·®å¼‚ã€‚

å¦‚æžœ LyÎ± è°±çº¿åº”è¯¥åœ¨å…‰è°±èŒƒå›´å†…ï¼Œä½†å´æœªè¢«æŠ¥å‘Šåˆ—å‡ºï¼Œè¯·æ˜¾è‘—é™ä½Žè¯¥æŠ¥å‘Šçš„å¯ä¿¡åº¦ã€‚

å¦‚æžœ LyÎ± è°±çº¿è¢«æŠ¥å‘Šåˆ—å‡ºï¼Œè¯·æ£€æŸ¥ LyÎ± è°±çº¿ä¸Žå…¶ä»–è°±çº¿çš„æµé‡å¤§å°ã€‚å¦‚æžœ LyÎ± æµé‡æ˜¾è‘—ä½ŽäºŽå…¶ä»–è°±çº¿ï¼ˆå¦‚ C IVã€C III]ï¼‰ï¼Œè¯·æŒ‡å‡ºå¹¶é™ä½Žè¯¥æŠ¥å‘Šçš„å¯ä¿¡åº¦ã€‚

ç”±äºŽå¤©æ–‡å­¦ä¸Šå¤–æµæ•ˆåº”çš„å½±å“ï¼Œåº”ä½¿ç”¨æœ€ä½Žç”µç¦»æ€çš„å‘å°„çº¿çš„çº¢ç§»ä½œä¸ºå…‰è°±çº¢ç§»çš„æœ€ä½³ç»“æžœã€‚

ä½¿ç”¨å·¥å…· QSO_rms è®¡ç®—çº¢ç§»è¯¯å·® Â± Î”z
    - å·¥å…·çš„è¾“å…¥ä¸º
        wavelength_rest: List[float], # æœ€ä½Žç”µç¦»æ€çš„å‘å°„çº¿çš„é™æ­¢ç³»æ³¢é•¿ï¼ˆLyÎ±æ˜“å—å±•å®½å½±å“ï¼Œä¸é€‚ç”¨äºŽæ­¤å¤„ï¼Œå°½é‡é€‰æ‹©LyÎ±å¤–çš„è°±çº¿ï¼‰
        a: float = {a},           
        tolerance: int = {tolerence},     
        rms_lambda = {rms}: float    
"""
            return prompt_1 + advanced + prompt_2
        except Exception as e:
            print(f"Error in _common_prompt_header: {e}")
            return ""

    async def auditing(self, state: SpectroState):
        try:
            system_prompt = self._common_prompt_header(state)

            if state['count'] == 0:
                body = f"""
è¯·å¯¹è¿™ä»½åˆ†æžæŠ¥å‘Šè¿›è¡Œæ£€æŸ¥ã€‚
"""
            elif state['count']: 
                auditing_history = state['auditing_history_QSO'][-1] 
                auditing_history_json = json.dumps(auditing_history, ensure_ascii=False)
                response_history = state['refine_history_QSO'][-1]
                response_history_json = json.dumps(response_history, ensure_ascii=False)

                body = f"""
ä½ å¯¹è¿™ä»½åˆ†æžæŠ¥å‘Šçš„æœ€æ–°è´¨ç–‘ä¸º
{auditing_history_json}

å…¶ä»–åˆ†æžå¸ˆçš„å›žç­”ä¸º
{response_history_json}

è¯·å›žåº”å…¶ä»–åˆ†æžå¸ˆçš„å›žç­”ï¼Œå¹¶ç»§ç»­è¿›è¡Œå®¡æŸ¥ã€‚
"""
            user_prompt = body
            response = await self.call_llm_with_context(system_prompt, user_prompt, parse_json=False, description="æŠ¥å‘Šå®¡æŸ¥")
            state['auditing_history_QSO'].append(response)
        except Exception as e:
            print(f"Error in auditing: {e}")

    async def run(self, state: SpectroState) -> SpectroState:
        if state['preliminary_classification']['type'] == "QSO":
            await self.auditing(state)
        return state


# # ---------------------------------------------------------
# # 4. Reflective Analyst â€” è‡ªç”±å›žåº”å®¡æŸ¥å¹¶æ”¹è¿›
# # ---------------------------------------------------------
class SpectralRefinementAssistant(BaseAgent):
    """æ”¹è¿›è€…ï¼šå›žåº”å®¡æŸ¥å¹¶æ”¹è¿›åˆ†æž"""

    def __init__(self, mcp_manager: MCPManager):
        super().__init__(
            agent_name='Spectral Refinement Assistant',
            mcp_manager=mcp_manager
        )

    def _common_prompt_header(self, state) -> str:
        try:
            peaks_info = [
                {
                    "wavelength": pe.get('wavelength'),
                    "flux": pe.get('mean_flux'),
                    "width": pe.get('width_mean'),
                    "prominance": pe.get('max_prominence'),
                    "seen_in_global_scales_of_sigma": pe.get('max_global_sigma_seen', None),
                    "describe": pe.get('describe')
                }
                for pe in state.get('cleaned_peaks', [])[:15]
            ]
            peak_json = json.dumps(peaks_info, ensure_ascii=False)

            trough_info = [
                {
                    "wavelength": tr.get('wavelength'),
                    "flux": tr.get('mean_flux'),
                    "width": tr.get('width_mean'),
                    "seen_in_scales_of_sigma": tr.get('seen_in_scales_of_sigma')
                }
                for tr in state.get('cleaned_troughs', [])[:15]
            ]
            trough_json = json.dumps(trough_info, ensure_ascii=False)
            rule_analysis = "\n\n".join(str(item) for item in state['rule_analysis_QSO'])
            a = state["pixel_to_value"]["x"]["a"]
            rms = state["pixel_to_value"]["x"]["rms"]
            tolerence = getenv_int("TOL_PIXELS", 10)
            prompt_1 = f"""
ä½ æ˜¯ä¸€ä½å…·å¤‡åæ€èƒ½åŠ›çš„ã€å¤©æ–‡å­¦å…‰è°±åˆ†æžå¸ˆã€‘ã€‚

ä»»åŠ¡ç›®æ ‡ï¼š
- é˜…è¯»å¹¶ç†è§£ä»–äººçš„å…‰è°±åˆ†æžæŠ¥å‘Š
- é˜…è¯»å¹¶ç†è§£å®¡æŸ¥å®˜æå‡ºçš„åé¦ˆ
- å¯¹è‡ªèº«æˆ–ä»–äººå…ˆå‰çš„åˆ†æžè¿›è¡Œæ”¹è¿›
- æå‡ºæ–°çš„è§£é‡Šæˆ–ä¿®æ­£ç»“è®º

å·¥ä½œåŽŸåˆ™ï¼š
- è®¤çœŸå›žåº”æ¯æ¡åé¦ˆï¼Œé€ä¸€è¯´æ˜Žæ”¹è¿›ä¹‹å¤„
- å¦‚æžœè®¤ä¸ºåŽŸç»“è®ºæ­£ç¡®ï¼Œéœ€ç»™å‡ºå……åˆ†ç†ç”±
- æœ€ç»ˆè¾“å‡ºä¸€ä¸ªæ›´ä¸¥è°¨ã€å®Œå–„çš„åˆ†æžç‰ˆæœ¬
- æ¶‰åŠçº¢ç§»å’Œå…‰è°±è§‚æµ‹æ³¢é•¿çš„è®¡ç®—å¿…é¡»ä½¿ç”¨å·¥å…· calculate_redshift å’Œ  predict_obs_wavelengthã€‚ä¸å…è®¸è‡ªè¡Œè®¡ç®—ã€‚

è¾“å‡ºè¦æ±‚ï¼š
- è¯·è¾“å‡ºè¯´æ˜Žæ€§çš„è¯­è¨€
- åˆ—å‡ºæ”¶åˆ°çš„åé¦ˆåŠå¯¹åº”å›žåº”
- æä¾›æ”¹è¿›åŽçš„å…‰è°±åˆ†æžæ€»ç»“
- è¯´æ˜Žä¿®æ”¹å†…å®¹åŠå…¶ç§‘å­¦åˆç†æ€§

å·²çŸ¥ï¼šç»¼åˆåŽŸæ›²çº¿å’Œ sigma=2ã€sigma=4ã€sigma=16 ä¸‰æ¡é«˜æ–¯å¹³æ»‘æ›²çº¿ï¼Œä½¿ç”¨ scipy å‡½æ•°è¿›è¡Œäº†å³°/è°·è¯†åˆ«ã€‚
å…³äºŽå³°/è°·çš„è®¨è®ºä»¥ä»¥ä¸‹æ•°æ®ä¸ºå‡†ï¼š
- ä»£è¡¨æ€§çš„å‰ 10 æ¡å‘å°„çº¿ï¼š
{peak_json}
- å¯èƒ½çš„å¸æ”¶çº¿ï¼š
{trough_json}

å…¶ä»–åˆ†æžå¸ˆç»™å‡ºçš„å…‰è°±åˆ†æžæŠ¥å‘Šä¸ºï¼š

{rule_analysis}

è¯¥æŠ¥å‘Šåœ¨çº¢ç§»è®¡ç®—æ—¶ä¿ç•™äº† 3 ä½å°æ•°ã€‚

è¯¥å…‰è°±çš„æ³¢é•¿èŒƒå›´æ˜¯{state['spectrum']['new_wavelength'][0]} Ã… åˆ° {state['spectrum']['new_wavelength'][-1]} Ã…ã€‚
"""
            band_name = state['band_name']
            band_wavelength = state['band_wavelength']
            if band_name: 
                overlap_regions = find_overlap_regions(band_name, band_wavelength)
                wws = np.max([wp.get('width_mean') for wp in state.get('wiped_peaks', [])[:5]])
                for key in overlap_regions:
                    range = overlap_regions[key]
                    overlap_regions[key] = [range[0]-wws, range[1]+wws] # Broaden the overlap regions to make sure LLM won't miss them
                overlap_regions_json = json.dumps(overlap_regions, ensure_ascii=False)
                wiped = [
                    {
                        "wavelength": wp.get('wavelength'),
                        "flux": wp.get('mean_flux'),
                        "width": wp.get('width_mean'),
                        # "seen_in_scales_of_sigma": wp.get('seen_in_scales_of_sigma')
                    }
                    for wp in state.get('wiped_peaks', [])[:5]
                ]
                wiped_json = json.dumps(wiped, ensure_ascii=False)
                advanced = f"""å¦‚æžœæŠ¥å‘Šä¸­çš„å³°å€¼è½åœ¨ä»¥ä¸‹åŒºé—´é™„è¿‘\n    {overlap_regions_json}\nåˆ™å³°å€¼å¯èƒ½è¢«å½“ä½œå™ªå£°ä¿¡å·æ¸…é™¤ã€‚è¿™äº›å³°å€¼æ˜¯ï¼š\n      {wiped_json}\nè¯·æ³¨æ„è€ƒå¯Ÿè¿™äº›å³°å€¼ä½œä¸º C IV æˆ– C III] çš„å¯èƒ½æ€§"""
            else:
                advanced = ""

            prompt_2 = f"""

æˆ‘å¸Œæœ›å…‰è°±åˆ†æžæŠ¥å‘Šèƒ½å¤Ÿå°½å¯èƒ½å¥½åœ°åŒ¹é… LyÎ±ã€C IVã€C III]ã€Mg II ç­‰å…¸åž‹å‘å°„çº¿ï¼Œä½†ä¹Ÿå…è®¸åœ¨ç”±äºŽå…‰è°±è¾¹ç¼˜çš„ä¿¡å·æ®‹ç¼ºæˆ–ä¿¡å™ªæ¯”ä¸ä½³å¯¼è‡´éƒ¨åˆ†å‘å°„çº¿ä¸å¯è§ã€‚

åŒæ—¶ï¼Œåœ¨ä¿¡å™ªæ¯”ä¸ä½³æ—¶ï¼Œå¯»æ‰¾è°±çº¿çš„ç®—æ³•ä¹Ÿä¼šå—åˆ°å½±å“ï¼Œå› æ­¤ä¹Ÿå…è®¸çº¿å®½ä¸ŽæœŸæœ›å­˜åœ¨ä¸€å®šçš„çš„å·®å¼‚ã€‚

å¦‚æžœ LyÎ± è°±çº¿åº”è¯¥åœ¨å…‰è°±èŒƒå›´å†…ï¼Œä½†å´æœªè¢«æŠ¥å‘Šåˆ—å‡ºï¼Œè¯·æ˜¾è‘—é™ä½Žè¯¥æŠ¥å‘Šçš„å¯ä¿¡åº¦ã€‚

å¦‚æžœ LyÎ± è°±çº¿è¢«æŠ¥å‘Šåˆ—å‡ºï¼Œè¯·æ£€æŸ¥ LyÎ± è°±çº¿ä¸Žå…¶ä»–è°±çº¿çš„æµé‡å¤§å°ã€‚å¦‚æžœ LyÎ± æµé‡æ˜¾è‘—ä½ŽäºŽå…¶ä»–è°±çº¿ï¼ˆå¦‚ C IVã€C III]ï¼‰ï¼Œè¯·æŒ‡å‡ºå¹¶é™ä½Žè¯¥æŠ¥å‘Šçš„å¯ä¿¡åº¦ã€‚

ç”±äºŽå¤©æ–‡å­¦ä¸Šå¤–æµæ•ˆåº”çš„å½±å“ï¼Œåº”ä½¿ç”¨æœ€ä½Žç”µç¦»æ€çš„å‘å°„çº¿çš„çº¢ç§»ä½œä¸ºå…‰è°±çº¢ç§»çš„æœ€ä½³ç»“æžœï¼ˆLyÎ±æ˜“å—å±•å®½å½±å“ï¼Œä¸é€‚ç”¨äºŽæ­¤å¤„ï¼Œå°½é‡é€‰æ‹©LyÎ±å¤–çš„è°±çº¿ï¼‰ã€‚

ä½¿ç”¨å·¥å…· QSO_rms è®¡ç®—çº¢ç§»è¯¯å·® Â± Î”z
    - å·¥å…·çš„è¾“å…¥ä¸º
        wavelength_rest: List[float], # æœ€ä½Žç”µç¦»æ€çš„å‘å°„çº¿çš„é™æ­¢ç³»æ³¢é•¿
        a: float = {a},           
        tolerance: int = {tolerence},     
        rms_lambda = {rms}: float 
"""
            return prompt_1 + advanced + prompt_2
        except Exception as e:
            logging.error(f"Error in _common_prompt_header: {e}")
            raise e


    async def refine(self, state: SpectroState):
        try:
            system_prompt = self._common_prompt_header(state)
            auditing_history = state['auditing_history_QSO'][-1]
            auditing_history_json = json.dumps(auditing_history, ensure_ascii=False)
            body = f"""
è´Ÿè´£æ ¸éªŒæŠ¥å‘Šçš„å®¡æŸ¥åˆ†æžå¸ˆç»™å‡ºçš„æœ€æ–°å»ºè®®ä¸º
{auditing_history_json}

è¯·å¯¹å»ºè®®è¿›è¡Œå›žåº”ã€‚
"""
            user_prompt = body
            response = await self.call_llm_with_context(system_prompt, user_prompt, parse_json=False, description="å›žåº”å®¡æŸ¥")
            state['refine_history_QSO'].append(response)
        except Exception as e:
            logging.error(f"Error in refine: {e}")
            raise e

    async def run(self, state: SpectroState) -> SpectroState:
        try:
            if state['preliminary_classification']['type'] == "QSO":
                await self.refine(state)
            return state
        except Exception as e:
            import traceback
            print("âŒ An error occurred during spectral analysis:")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            print("Full traceback:")
            traceback.print_exc()
            # å¯é€‰ï¼šè¿”å›žå½“å‰çŠ¶æ€æˆ–æŠ›å‡ºå¼‚å¸¸
            raise  # å¦‚æžœä½ å¸Œæœ›è°ƒç”¨è€…ä¹Ÿèƒ½æ•èŽ·è¯¥å¼‚å¸¸


# ---------------------------------------------------------
# ðŸ§© 5. Host Integrator â€” æ±‡æ€»ä¸Žæ€»ç»“å¤šæ–¹è§‚ç‚¹
# ---------------------------------------------------------
class SpectralSynthesisHost(BaseAgent):
    """æ±‡æ€»ä¸»æŒäººï¼šæ•´åˆå¤šAgentçš„åˆ†æžä¸Žç»“è®º"""

    def __init__(self, mcp_manager: MCPManager):
        super().__init__(
            agent_name='Spectral Synthesis Host',
            mcp_manager=mcp_manager
        )

    def get_system_prompt(self) -> str:
        return f"""
ä½ æ˜¯ä¸€ä½è´Ÿè´£ç»Ÿç­¹çš„ã€å¤©æ–‡å­¦å…‰è°±åˆ†æžä¸»æŒäººã€‘ã€‚

ä»»åŠ¡ç›®æ ‡ï¼š
- æ±‡æ€»è§†è§‰åˆ†æžå¸ˆã€è§„åˆ™åˆ†æžå¸ˆã€å®¡æŸ¥å®˜å’Œå†åˆ†æžå¸ˆçš„æ‰€æœ‰è¾“å‡º
- ç»¼åˆä¸åŒè§’åº¦çš„ç»“è®ºï¼Œå½¢æˆæœ€ç»ˆçš„å…‰è°±è§£é‡Š
- æ¸…æ¥šæŒ‡å‡ºå„æ–¹æ„è§çš„å·®å¼‚ä¸Žä¸€è‡´ç‚¹

å·¥ä½œåŽŸåˆ™ï¼š
- æ— éœ€è°ƒç”¨å·¥å…·
- ä¸ç›²ä»Žä»»ä½•å•ä¸€åˆ†æž
- ä¿æŒæ•´ä½“ç§‘å­¦æ€§ä¸Žé€»è¾‘ä¸€è‡´æ€§
- æœ€ç»ˆè¾“å‡ºå¿…é¡»å…·å¤‡å¯è¿½æº¯æ€§ï¼ˆè¯´æ˜Žæ¥è‡ªå“ªäº›agentçš„ä¾æ®ï¼‰

è¾“å‡ºè¦æ±‚ï¼š
- è¾“å‡ºè¯´æ˜Žæ€§æ–‡å­—
- è¾“å‡ºæ•°æ®ä¿ç•™ 3 ä½å°æ•°
- åªéœ€è¾“å‡ºåˆ†æžå†…å®¹ï¼Œæ— éœ€å£°æ˜Žå„æ®µåˆ†æžæ–‡å­—çš„æ¥æº
- ç»™å‡ºæœ€ç»ˆç»¼åˆç»“è®ºåŠå¯ä¿¡åº¦è¯„çº§ï¼ˆé«˜/ä¸­/ä½Žï¼‰
- å¦‚æžœä»å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œè¯·æ˜Žç¡®æŒ‡å‡º
- æŒ‰æ ¼å¼è¾“å‡ºã€‚ä¸è¦è¾“å‡ºå¤šä½™å†…å®¹
"""


    async def summary(self, state):
        try:
            preliminary_classification_json = json.dumps(state['preliminary_classification'], ensure_ascii=False)
            visual_interpretation_json = json.dumps(state['visual_interpretation'], ensure_ascii=False)
        except Exception as e:
            print("âŒ An error occurred during spectral analysis:")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            raise

        prompt_1 = f"""

å¯¹å…‰è°±çš„è§†è§‰æè¿°
{visual_interpretation_json}

å…‰è°±çš„åˆæ­¥åˆ†ç±»
{preliminary_classification_json}
"""
        system_prompt = self.get_system_prompt() + prompt_1

        if state['preliminary_classification']['type'] == "QSO":
            rule_analysis_QSO = "\n\n".join(str(item) for item in state['rule_analysis_QSO'])
            rule_analysis_QSO_json = json.dumps(rule_analysis_QSO, ensure_ascii=False)
            auditing_QSO = "\n\n".join(str(item) for item in state['auditing_history_QSO'])
            auditing_QSO_json = json.dumps(auditing_QSO, ensure_ascii=False)
            refine_QSO = "\n\n".join(str(item) for item in state['refine_history_QSO'])
            refine_QSO_json = json.dumps(refine_QSO, ensure_ascii=False)
            prompt_2 = f"""

è§„åˆ™åˆ†æžå¸ˆçš„è§‚ç‚¹ï¼š
{rule_analysis_QSO_json}

å®¡æŸ¥åˆ†æžå¸ˆçš„è§‚ç‚¹ï¼š
{auditing_QSO_json}

å®Œå–„åˆ†æžå¸ˆçš„è§‚ç‚¹ï¼š
{refine_QSO_json}
"""
            prompt_3 = f"""

è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š

- å…‰è°±çš„è§†è§‰ç‰¹ç‚¹
- åˆ†æžæŠ¥å‘Šï¼ˆç»¼åˆè§„åˆ™åˆ†æžå¸ˆã€å®¡æŸ¥åˆ†æžå¸ˆå’Œå®Œå–„åˆ†æžå¸ˆçš„æ‰€æœ‰è§‚ç‚¹ï¼Œé€ä¸ª Step è¿›è¡Œç»“æž„åŒ–è¾“å‡ºï¼‰
    - Step 1
    - Step 2
    - Step 3
    - Step 4
- ç»“è®º
    - è¯¥å¤©ä½“çš„å¤©ä½“ç±»åž‹ï¼ˆGalaxy è¿˜æ˜¯ QSOï¼‰
    - å¦‚æžœå¤©ä½“æ˜¯QSOï¼Œè¾“å‡ºçº¢ç§» z Â± Î”z
    - è®¤è¯å‡ºçš„è°±çº¿ï¼ˆè¾“å‡º è°±çº¿å - Î»_rest - Î»_obs - çº¢ç§»ï¼‰
    - å…‰è°±çš„ä¿¡å™ªæ¯”å¦‚ä½•
    - åˆ†æžæŠ¥å‘Šçš„å¯ä¿¡åº¦è¯„åˆ†ï¼ˆ0-4ï¼‰
        å¦‚æžœèƒ½è®¤è¯å‡º 2 æ¡ä»¥ä¸Šçš„ä¸»è¦è°±çº¿ï¼ˆæŒ‡ LyÎ±, C IV, C III, Mg IIï¼‰ï¼Œåˆ™å¯ä¿¡åº¦ä¸º 3ï¼›
        èƒ½è®¤è¯å‡º 1 æ¡ä¸»è¦è°±çº¿ï¼Œä¸”æœ‰å…¶ä»–è¾ƒå¼±çš„ç‰¹å¾ï¼Œåˆ™å¯ä¿¡åº¦ä¸º 2ï¼›
        èƒ½è®¤è¯å‡º 1 æ¡ä¸»è¦è°±çº¿ï¼Œä½†æ²¡æœ‰å…¶ä»–ç‰¹å¾è¾…åŠ©åˆ¤æ–­ï¼Œåˆ™å¯ä¿¡åº¦ä¸º 1ï¼›
        å…‰è°±ä¿¡å™ªæ¯”æžä½Žï¼Œå«ä¹‰è¿›è¡ŒæŽ¨æ–­ï¼Œåˆ™å¯ä¿¡åº¦ä¸º 0.
    - æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥åˆ¤æ–­ï¼ˆå¯ä¿¡åº¦ä¸º 0-2 æ—¶å¿…é¡»å¼•å…¥äººå·¥åˆ¤æ–­ã€‚ä¿¡å™ªæ¯”è¾ƒä½Žä¸”æ—  LyÎ± æ—¶å¿…é¡»å¼•å…¥äººå·¥åˆ¤æ–­ã€‚å…¶ä½™æƒ…å†µè‡ªè¡Œå†³ç­–ã€‚ï¼‰
"""
            user_prompt = prompt_2 + prompt_3
        else:
            user_prompt = f"""
è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š

- å…‰è°±çš„è§†è§‰ç‰¹ç‚¹
- ç»“è®º
    - è¯¥å¤©ä½“çš„å¤©ä½“ç±»åž‹ï¼ˆGalaxy è¿˜æ˜¯ QSOï¼‰
    - å¦‚æžœå¤©ä½“æ˜¯QSOï¼Œè¾“å‡ºçº¢ç§» z Â± Î”z
    - è®¤è¯å‡ºçš„è°±çº¿ï¼ˆè¾“å‡º è°±çº¿å - Î»_rest - Î»_obs - çº¢ç§»ï¼‰
    - å…‰è°±çš„ä¿¡å™ªæ¯”å¦‚ä½•
    - åˆ†æžæŠ¥å‘Šçš„å¯ä¿¡åº¦è¯„åˆ†ï¼ˆ0-4ï¼‰
        å¦‚æžœè®¤è¯ä¸º Galaxyï¼Œåˆ™å¯ä¿¡åº¦ä¸º 2ï¼›å¦åˆ™ä¸º 0ã€‚
    - æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥åˆ¤æ–­ï¼ˆå¦‚æžœç±»åž‹ä¸ºGalaxyï¼Œåˆ™å¿…é¡»è¦æ±‚äººå·¥ä»‹å…¥åˆ¤æ–­ï¼‰
"""
        response = await self.call_llm_with_context(system_prompt, user_prompt, parse_json=False, description="æ€»ç»“")
        state['summary'] = response
    async def in_brief(self, state):
        summary_json = json.dumps(state['summary'], ensure_ascii=False)
        prompt_type = f"""
ä½ æ˜¯ä¸€ä½è´Ÿè´£ç»Ÿç­¹çš„ã€å¤©æ–‡å­¦å…‰è°±åˆ†æžä¸»æŒäººã€‘

ä½ å·²ç»å¯¹ä¸€å¼ å¤©æ–‡å­¦å…‰è°±åšäº†æ€»ç»“
{summary_json}

- è¯·è¾“å‡º **ç»“è®º** éƒ¨åˆ†ä¸­çš„ **å¤©ä½“ç±»åž‹**ï¼ˆä»Žè¿™ä¸‰ä¸ªè¯è¯­ä¸­é€‰æ‹©ï¼šStar, Galaxy, QSOï¼‰

- è¾“å‡ºæ ¼å¼ä¸º str
- ä¸è¦è¾“å‡ºå…¶ä»–ä¿¡æ¯
"""
        response_type = await self.call_llm_with_context('', prompt_type, parse_json=False, description="æ€»ç»“")
        state['in_brief']['type'] = response_type

        prompt_redshift = f"""
ä½ æ˜¯ä¸€ä½è´Ÿè´£ç»Ÿç­¹çš„ã€å¤©æ–‡å­¦å…‰è°±åˆ†æžä¸»æŒäººã€‘

ä½ å·²ç»å¯¹ä¸€å¼ å¤©æ–‡å­¦å…‰è°±åšäº†æ€»ç»“
{summary_json}

è¯·è¾“å‡º **ç»“è®º** éƒ¨åˆ†ä¸­çš„ **çº¢ç§» z**ï¼ˆä¸éœ€è¦è¾“å‡º Â± Î”zï¼‰

- è¾“å‡ºæ ¼å¼ä¸º float æˆ– None
- ä¸è¦è¾“å‡ºå…¶ä»–ä¿¡æ¯
"""
        response_redshift = await self.call_llm_with_context('', prompt_redshift, parse_json=False, description="æ€»ç»“")
        state['in_brief']['redshift'] = response_redshift

        prompt_rms = f"""
ä½ æ˜¯ä¸€ä½è´Ÿè´£ç»Ÿç­¹çš„ã€å¤©æ–‡å­¦å…‰è°±åˆ†æžä¸»æŒäººã€‘

ä½ å·²ç»å¯¹ä¸€å¼ å¤©æ–‡å­¦å…‰è°±åšäº†æ€»ç»“
{summary_json}

è¯·è¾“å‡º **ç»“è®º** éƒ¨åˆ†ä¸­çš„ **çº¢ç§»è¯¯å·® Î”z**ï¼ˆä¸éœ€è¦è¾“å‡º zï¼‰

- è¾“å‡ºæ ¼å¼ä¸º float æˆ– None
- ä¸è¦è¾“å‡ºå…¶ä»–ä¿¡æ¯
"""
        response_rms = await self.call_llm_with_context('', prompt_rms, parse_json=False, description="æ€»ç»“")
        state['in_brief']['rms'] = response_rms

        prompt_human = f"""
ä½ æ˜¯ä¸€ä½è´Ÿè´£ç»Ÿç­¹çš„ã€å¤©æ–‡å­¦å…‰è°±åˆ†æžä¸»æŒäººã€‘

ä½ å·²ç»å¯¹ä¸€å¼ å¤©æ–‡å­¦å…‰è°±åšäº†æ€»ç»“
{summary_json}

è¯·è¾“å‡º **ç»“è®º** éƒ¨åˆ†ä¸­çš„ **æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥åˆ¤æ–­**

- ä»…è¾“å‡ºâ€œæ˜¯â€æˆ–â€œå¦â€
- è¾“å‡ºæ ¼å¼ä¸º str
- ä¸è¦è¾“å‡ºå…¶ä»–ä¿¡æ¯
"""
        response_human = await self.call_llm_with_context('', prompt_human, parse_json=False, description="æ€»ç»“")
        state['in_brief']['human'] = response_human
    
    async def run(self, state: SpectroState) -> SpectroState:
        try:
            await self.summary(state)
            await self.in_brief(state)
            return state
        except Exception as e:
            import traceback
            print("âŒ An error occurred during spectral analysis:")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            print("Full traceback:")
            traceback.print_exc()
            # å¯é€‰ï¼šè¿”å›žå½“å‰çŠ¶æ€æˆ–æŠ›å‡ºå¼‚å¸¸
